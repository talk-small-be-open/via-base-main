user	www-data;
worker_processes auto;
pid /var/run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;


events {
	worker_connections 768;
	# multi_accept on;
}


http {

		##
		# Basic Settings
		##

		sendfile on;
		tcp_nopush on;
		tcp_nodelay on;
		keepalive_timeout 65;
		types_hash_max_size 2048;

		# also in ngxblocker: server_names_hash_bucket_size 64;

		include /etc/nginx/mime.types;
		default_type application/octet-stream;

		##
		# Logging Settings
		##

		access_log /var/log/nginx/access.log;
		error_log /var/log/nginx/error.log;

		##
		# Gzip Settings
		##

		gzip on; # TBD: Needed?
		
		client_max_body_size 0; # disable limit

		# We use HAProxy as an intermediate load balancer, because its far better than nginx.
		# And, important: It queues the request per backend server instance, since we allow only
		# one single request per GemStone gem.
		upstream haproxy_upstream { 
		  server 127.0.0.1:9000; 
			keepalive 64; 
		}


		# Used for peerJS
		map $http_upgrade $connection_upgrade {
				default upgrade;
				'' close;
		}

		#
		# Limits
		#
		# Consider: Seaside uses 2 requests per hit (1 for redirect message)
		
		# Define request limit, used for demo and app.
		
		limit_req_zone $binary_remote_addr zone=demo:5m rate=2r/s;

		# Consider: Users are often coming from the same class room, therefore same IP! Dont limit to hard!
		# What we want is to ban idiots
		limit_req_zone $binary_remote_addr zone=app:10m rate=20r/s;

		# We need to manually re-add this. ngxblocker out-comments this in /etc/nginx/conf.d/botblocker-nginx-settings.conf
		# because of conflict avoidance. But zone flood needs to exist.
		limit_req_zone $binary_remote_addr zone=flood:50m rate=90r/s;


		# limit_conn_zone $binary_remote_addr zone=app:10m
		limit_req_status 509; # = "Bandwidth exceeded"



		##
		# Virtual Host Configs
		##

		include /etc/nginx/conf.d/*.conf;
		include /etc/nginx/sites-enabled/*;


}
